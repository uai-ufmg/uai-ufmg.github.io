---
title: 'Unraveling SimCLR: Self-Supervised Learning and Contrastive Visual Representations'

show_breadcrumb: true

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - henriqueMagalhaes

date: '2024-06-22T00:00:00Z'
doi: ''

publishDate: '2024-06-22T00:00:00Z'

publication_types: ['paper-conference']

abstract: SimCLR (Simple Framework for Contrastive Learning of Visual Representations) is a neural network algorithm that trains a model to identify similar images without using pre-defined similarity labels. Therefore, it is called self-supervised. SimCLR creates a latent representation for each image and uses learning based on the contrast between an anchor image and its augmented versions. In this presentation, we will explore self-supervised algorithms, with an emphasis on the SimCLR method. We will discuss the key components of SimCLR, including the data augmentation stage, the role of the encoder, and the projection head. Additionally, we will explain the theory behind contrastive learning and how it contributes to creating efficient representation models.

summary: Seminar presented by Henrique Magalh√£es on SimCLR (22/06/2024 at 2 PM).

tags: []

featured: true

image:
  focal_point: ''
  preview_only: false

projects:
  - example

slides: example
---

<p>SimCLR (Simple Framework for Contrastive Learning of Visual Representations) is a neural network algorithm that trains a model to identify similar images without using pre-defined similarity labels. Therefore, it is called self-supervised. SimCLR creates a latent representation for each image and uses learning based on the contrast between an anchor image and its augmented versions. In this presentation, we will explore self-supervised algorithms, with an emphasis on the SimCLR method. We will discuss the key components of SimCLR, including the data augmentation stage, the role of the encoder, and the projection head. Additionally, we will explain the theory behind contrastive learning and how it contributes to creating efficient representation models.</p>